{
    "architectures": [
        "OPTForCausalLM"
    ],
    "model_type": "opt",
    "activation_dropout": 0.0,
    "activation_function": "silu",
    "attention_dropout": 0.0,
    "do_layer_norm_before": true,
    "dropout": 0.1,
    "ffn_dim": 1536,
    "hidden_size": 576,
    "init_std": 0.02,
    "layerdrop": 0.0,
    "max_position_embeddings": 2048,
    "num_attention_heads": 9,
    "num_hidden_layers": 30,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "vocab_size": 50272,
    "word_embed_proj_dim": 576
}