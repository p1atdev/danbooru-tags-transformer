{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ndart.modeling_ndart import NDartForConditionalGeneration\n",
    "from ndart.processing_ndart import NDartProcessor\n",
    "from ndart.configuration_ndart import NDartConfig\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForTextEncoding,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDartForConditionalGeneration(\n",
      "  (encoder_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(250037, 384, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 384)\n",
      "      (token_type_embeddings): Embedding(2, 384)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder_model): LlamaForCausalLM(\n",
      "    (model): LlamaModel(\n",
      "      (embed_tokens): Embedding(37555, 768, padding_idx=2)\n",
      "      (layers): ModuleList(\n",
      "        (0-7): 8 x LlamaDecoderLayer(\n",
      "          (self_attn): LlamaSdpaAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=96, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=96, bias=False)\n",
      "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
      "            (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
      "            (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm((768,), eps=1e-05)\n",
      "          (post_attention_layernorm): LlamaRMSNorm((768,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm((768,), eps=1e-05)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=37555, bias=False)\n",
      "  )\n",
      "  (projection): NDartEmbeddingProjector(\n",
      "    (linear_1): Linear(in_features=384, out_features=768, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "    (linear_2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder_model = \"intfloat/multilingual-e5-small\"\n",
    "decoder_model = \"p1atdev/dart-v3-llama-8L-241018_241020-sft-use-group\"\n",
    "bert = AutoModelForTextEncoding.from_pretrained(encoder_model)\n",
    "dart = AutoModelForCausalLM.from_pretrained(decoder_model)\n",
    "processor = NDartProcessor(\n",
    "    encoder_tokenizer=AutoTokenizer.from_pretrained(encoder_model),\n",
    "    decoder_tokenizer=AutoTokenizer.from_pretrained(decoder_model),\n",
    "    natural_token=\"<|natural|>\",\n",
    ")\n",
    "\n",
    "model = NDartForConditionalGeneration._from_config(\n",
    "    NDartConfig(\n",
    "        natural_config=bert.config,\n",
    "        tag_config=dart.config,\n",
    "        natural_token_index=processor.natural_token_id,\n",
    "    )\n",
    ")\n",
    "model.encoder_model = bert\n",
    "model.decoder_model = dart\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_prompt = [\n",
    "    \"<|bos|><projection><|natural|></projection>\",\n",
    "    \"<projection><|natural|></projection><|translate:exact|><|input_end|>\",\n",
    "]\n",
    "natural_text = [\n",
    "    \"an image\",\n",
    "    \"黒髪ロング猫耳美少女JK\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input_ids: tensor([[ 0, 65, 60, 60, 60, 60, 67,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [65, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 67, 71, 40]])\n",
      "Encoder embeddings: torch.Size([2, 12, 384]) tensor([[0.1989, 0.0757, 0.1789, 0.1089, 0.2332, 0.2270, 0.2442, 0.2437, 0.2468,\n",
      "         0.2508, 0.2663, 0.2650],\n",
      "        [0.2803, 0.2220, 0.3636, 0.2078, 0.0874, 0.4443, 0.3044, 0.2226, 0.3171,\n",
      "         0.4064, 0.4431, 0.2801]])\n",
      "Natural attention mask: tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Projected embeddings: tensor([[ 0.0603,  0.0737,  0.0830,  0.0786,  0.0654,  0.0620,  0.0645,  0.0669,\n",
      "          0.0659,  0.0659,  0.0649,  0.0635],\n",
      "        [ 0.0332, -0.0253,  0.0244,  0.0299,  0.0270,  0.0084,  0.0425,  0.0276,\n",
      "          0.0240,  0.0254,  0.0270,  0.0332]], dtype=torch.bfloat16)\n",
      "Decoder input_ids: tensor([[ 0, 65, 60, 60, 60, 60, 67,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [65, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 67, 71, 40]])\n",
      "Decoder embeddings: torch.Size([2, 16, 768]) tensor([[ 0.0181,  0.0101,  0.0040,  0.0040,  0.0040,  0.0040,  0.0205, -0.0042,\n",
      "         -0.0042, -0.0042, -0.0042, -0.0042, -0.0042, -0.0042, -0.0042, -0.0042],\n",
      "        [ 0.0101,  0.0040,  0.0040,  0.0040,  0.0040,  0.0040,  0.0040,  0.0040,\n",
      "          0.0040,  0.0040,  0.0040,  0.0040,  0.0040,  0.0205, -0.0031, -0.0378]])\n",
      "Replaced decoder embeddings: torch.Size([2, 16, 768]) tensor([[ 0.0181,  0.0101,  0.0603,  0.0737,  0.0830,  0.0786,  0.0205, -0.0042,\n",
      "         -0.0042, -0.0042, -0.0042, -0.0042, -0.0042, -0.0042, -0.0042, -0.0042],\n",
      "        [ 0.0101,  0.0332, -0.0253,  0.0244,  0.0299,  0.0270,  0.0084,  0.0425,\n",
      "          0.0276,  0.0240,  0.0254,  0.0270,  0.0332,  0.0205, -0.0031, -0.0378]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cpu\"):\n",
    "        inputs = processor(\n",
    "            natural_text=natural_text,\n",
    "            tag_text=tag_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Encoder input_ids: {inputs.input_ids}\",\n",
    "        )\n",
    "        encoder_embeds = model.encoder_model(\n",
    "            input_ids=inputs.encoder_input_ids,\n",
    "            attention_mask=inputs.encoder_attention_mask,\n",
    "        ).last_hidden_state\n",
    "        print(\n",
    "            f\"Encoder embeddings: {encoder_embeds.shape} {encoder_embeds[:, :, 0]}\",\n",
    "        )\n",
    "        print(\n",
    "            f\"Natural attention mask: {inputs.encoder_attention_mask}\",\n",
    "        )\n",
    "\n",
    "        projected_embeds = model.projection(encoder_embeds)\n",
    "        print(\n",
    "            f\"Projected embeddings: {projected_embeds[:, :, 0]}\",\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Decoder input_ids: {inputs.input_ids}\",\n",
    "        )\n",
    "        decoder_embeds = model.decoder_model.get_input_embeddings()(\n",
    "            inputs.input_ids,\n",
    "        )\n",
    "        print(\n",
    "            f\"Decoder embeddings: {decoder_embeds.shape} {decoder_embeds[:, :, 0]}\",\n",
    "        )\n",
    "\n",
    "        replaced_decoder_embeds = model._replace_natural_token_embeddings(\n",
    "            encoder_embeds=projected_embeds,\n",
    "            decoder_input_ids=inputs.input_ids,\n",
    "            decoder_embeds=decoder_embeds,\n",
    "            encoder_attention_mask=inputs.encoder_attention_mask,\n",
    "        )\n",
    "        print(\n",
    "            f\"Replaced decoder embeddings: {replaced_decoder_embeds.shape} {replaced_decoder_embeds[:, :, 0]}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 65, 60, 60, 60, 60, 67,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "        [65, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 67, 71, 40]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'encoder_input_ids': tensor([[    0,   142, 29569,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [    0,     6, 74496, 92356, 11119, 76337, 35076, 21645,  2655, 48701,\n",
       "         63859,     2]]), 'encoder_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(\n",
    "    natural_text=natural_text,\n",
    "    tag_text=tag_prompt,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amagi yukiko, :d, closed eyes, ^_^, round teeth, glasses, xd, profile, :o, beret, profile, profile, :o, profile, drill hair, facing viewer'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=16, use_cache=True)\n",
    "\n",
    "\", \".join(\n",
    "    [\n",
    "        token\n",
    "        for token in processor.batch_decode(outputs[0], skip_special_tokens=True)\n",
    "        if token != \"\"\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
